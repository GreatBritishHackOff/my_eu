{"datasets": {"cordis": {"columns": ["inwardCode", "projectTitle", "organisationName", "objective", "contribution", "totalCost", "acronym", "briefTitle", "teaser", "article", "projectUrl", "organizationUrl", "imagePath", "myEuId"], "data": [["0XZ", "The Open Pharmacological Concepts Triple Store", "OpenLink Group Ltd", "Drug discovery is data-hungry and all major pharmaceutical companies maintain extensive in-house instances of public data alongside internal. Analysis and hypothesis generation for drug-discovery projects requires assembly, overlay and comparison of data from many sources, requiring shared identifiers and common semantics. Expression profiles need to be overlaid with gene or pathway identifiers and reports on compound pharmacology. Alignment and integration of internal and public data and information sources is a significant effort and the process is repeated across companies, institutes and academic laboratories. This represents a significant waste and an opportunity cost.\n \nTo address these challenges, the Open PHACTS project will develop an open access innovation platform, Open Pharmacological Space (OPS), via a semantic web approach. OPS will comprise data, vocabularies and infrastructure needed to accelerate drug-oriented research. This semantic integration hub will remove key bottlenecks in small molecule drug discovery: disparate information sources, lack of standards and common identifiers, guided by well defined research questions from drug discovery. \n\nWorkflows for data capture, processing, interoperability, visualization, and chemogenomics will be developed creating a comprehensive Systems Chemical Biology Analysis Network. Security issues around proprietary data, shared via the Open PHACTS Discovery Platform and accessible for safe querying and reasoning will be properly addressed with expert trusted parties.\n\nThe Open PHACTS consortium comprises 20 European academic and SME partners, with leading experts in the fields of data mining, annotation, small molecule data storage and manipulation, target bioinformatics, RDF information handling, massive in silico reasoning and chemical biology. The 10 EFPIA members of Open PHACTS will contribute drug discovery expertise, data sets, software engineering and programming capacity to the project.", 335248.0, 16313907.0, "OPEN PHACTS", null, null, null, null, "http://www.openlinksw.com", null, "fp7_203694_998372404"], ["0XZ", "OntoWiki \u2013 Semantic Collaboration for Enterprise Knowledge Management, E-Learning and E-Tourism", "OpenLink Group Ltd", "The aim of this project is to prepare and start-up the commercial exploitation of the semantic collaboration software OntoWiki in the three different target markets, namely Enterprise Knowledge Management, semantically enhanced content management for E-Learning and E-Tourism. OntoWiki is a comprehensive open-source platform for social semantic collaboration. It is developed at Universit\u00e4t Leipzig and has a large and active user base. Within the course of the project OntoWiki will be further developed and adopted to the needs of the SME for exploitation of OntoWiki in the prospective target markets.", 256189.0, 1285281.0, "ONTOWIKI", null, null, null, "http://ontowiki.eu/Welcome", "http://www.openlinksw.com", null, "fp7_92171_998372404"], ["0XZ", "Making the Web an Exploratory for Geospatial Knowledge", "OpenLink Group Ltd", "The advent of the Data Web demonstrates how Web technologies can be employed to integrate dispersed, heterogeneous information. Most information \u2013 be it the product logistics status within supply chains, data warehouses of E-Commerce systems or offerings of the bakery shop next door \u2013 has a directly or indirectly associated spatial dimension. Integrating and efficiently using information with such a spatial dimension on the Web poses very particular challenges. First, we have to combine and reason about spatial and semantic features that use different vocabularies and representation techniques. Second, the wealth of pre-existing overlapping and complementary information sources in the spatial domain demands information management able to deal with billions of facts in a scalable manner. Third, only by dramatically lowering the entrance barriers extra value can be generated through the involvement of thousands of non-expert end users in authoring, curation and assessment. GeoKnow tackles these challenges and facilitates the transition from islands of isolated Geographic Information Systems (GIS) to a Web of interlinked Geographic Knowledge Systems (GKS). GeoKnow focuses on two complementary application scenarios: (1) spatial-semantic collaboration and data integration along value-chains in supplier and customers networks; and (2) spatial-semantic travel E-Commerce data management. GeoKnow will contribute to the evolution of the Web from a medium for information exchange to a medium for (spatial) knowledge integration. GeoKnow aims to help realizing network effects by increasing quality and coherence of the Spatial Data Web. For enterprises, the GeoKnow Generator will provide a cost-efficient way to integrate a variety of heterogeneous information sources within an Enterprise Data Intranet.", 362785.0, 3575810.0, "GeoKnow", null, null, null, null, "http://www.openlinksw.com", null, "fp7_106337_998372404"], ["0XZ", "Linked Data Benchmark Council", "OpenLink Group Ltd", "Non-relational data management is emerging as a critical need for the new data economy based on large, distributed, heterogeneous, and complexly structured data sets. This new data management paradigm also provides an opportunity for research results to impact young innovative companies working on new RDF and graph data management technologies to start playing a significant role in this new data economy.Standards and benchmarking are two of the most important factors for the development of new information technology, yet there is still no comprehensive suite of benchmarks and benchmarking practices for RDF and graph databases, nor is there an authority for setting benchmark definitions and auditing official results. Without them, the future development and uptake of these technologies is at risk by not providing industry with clear, user-driven targets for performance and functionality.The goal of the Linked Data Benchmark Council (LDBC) project is to create the first comprehensive suite of open, fair and vendor-neutral benchmarks for RDF/graph databases together with the LDBC foundation which will define processes for obtaining, auditing and publishing results. The core scientific innovation of LDBC is therefore to define meaningful benchmarks derived from a combination of actual usage scenarios combined with the technical insight of top database systems researchers and architects in the choke points of current technology. LDBC will bring together a broad community of researchers and RDF and graph database vendors to establish an independent authority, the LDBC foundation, responsible for specifying benchmarks, benchmarking procedures and verifying/publishing results. The forum created will become a long-surviving, industry supported association similar to the TPC. Vendors and user organisations will participate in order to influence benchmark design and to make use of the obvious marketing opportunities.", 318068.0, 2833769.0, "LDBC", null, null, null, null, "http://www.openlinksw.com", null, "fp7_105871_998372404"], ["0XZ", "Fusepool Publish-Process-Perform Platform for Linked Data", "OpenLink Group Ltd", "To make publishing and processing of linked data easy, the proposed project develops a set of integrated software components based on open-source Linked Data Platform best practices.  The tightly integrated components support the multilingual data value chain from data exploration (e.g. identifying structured and unstructured data sources), extraction (e.g. using named entity recognition, RDF conversion), enrichment (e.g. interlinking, crowdsourcing), and delivery (e.g. analytics, apps for desktop and mobile devices). These components run on an open-source data platform with various enterprise-grade storage solutions.The vision is to make publishing and reuse of linked data as easy as possible for the end user thanks to a thriving market economy with data publishers, developers, and consumers along the value chain. Making data reusable and interoperable within and outside the organization requires a fundamentally different ap-proach to 'storing' knowledge. 'The best name is probably a Logical Data Warehouse...because it focuses on the logic of information ...[for] giving integrated access to all forms of information assets.'  Only with integrated access to the data is it possible to have apps on top of that data that scale across single use cases and provide real added value.Fusepool LDAP (Linked Data Analytics Processing) derives its name from the idea of fusing and pooling linked data with analytical processing on top of it. Because linked data is multidimensional data, it lends itself to analytical processing such as consolidation (e.g. aggregation within a dimension), drill-down (e.g. navigating through the details), and slicing and dicing (e.g. viewing an aspect from different dimensions). However, an integrated publishing and processing workflow with integrated user interfaces is still missing. The lack of an integrated publishing and processing environment makes it difficult and time-consuming for data publishers and consumers to engage with linked data.", 161422.0, 1423479.0, "Fusepool P3", null, null, null, null, "http://www.openlinksw.com", null, "fp7_191690_998372404"], ["0XZ", "LOD2 - Creating Knowledge out of Interlinked Data", "OpenLink Group Ltd", "Over the last 3 years, the semantic web activity has gained momentum with the widespread publishing of structured data as RDF. The Linked Data paradigm has therefore evolved from a practical research idea into a very promising candidate for addressing one of the biggest challenges in the area of intelligent information management: the exploitation of the Web as a platform for data and information integration in addition to document search. To translate this initial success into a world-scale disruptive reality, encompassing the Web 2.0 world and enterprise data alike, the following research challenges need to be addressed: improve coherence and quality of data published on the Web, close the performance gap between relational and RDF data management, establish trust on the Linked Data Web and generally lower the entrance barrier for data publishers and users. With partners among those who initiated and strongly supported the Linked Open Data initiative, the LOD2 project aims at tackling these challenges by developing:1. enterprise-ready tools and methodologies for exposing and managing very large amounts of structured information on the Data Web,2. a testbed and bootstrap network of high-quality multi-domain, multi-lingual ontologies from sources such as Wikipedia and OpenStreetMap.3. machine learning algorithms for automatically enriching, repairing, interlinking and fusing data from the Web.4. standards and methods for reliably tracking provenance, ensuring privacy and data security as well as for assessing the quality of information.5. adaptive tools for searching, browsing, and authoring of Linked Data.We will integrate and syndicate linked data with large-scale, existing applications and showcase the benefits in the three application scenarios media &amp; publishing, corporate data intranets and e-government.", 731352.0, 8342462.0, "LOD2", null, null, null, null, "http://www.openlinksw.com", null, "fp7_95562_998372404"], ["0XZ", "Holistic Benchmarking of Big Linked Data", "OpenLink Group Ltd", "Linked Data has gained significant momentum over the last years. It is now used at industrial scale in many sectors in which an increasingly large amount of rapidly changing data needs to be processed. HOBBIT is an ambitious project that aims to push the development of Big Linked Data (BLD) processing solutions by providing a family of industry-relevant benchmarks for the BLD value chain through a generic evaluation platform. \nWe aim to make open deterministic benchmarks available to test the performance of existing systems and push the development of innovative industry-relevant solutions. The underlying data will mimic real industrial data assembled during the course of the project. At the beginning of the project, HOBBIT will work on roughly 1PB of real industry-relevant data from 4 different domains. The data will be extended through collaborations during the project. \nTo push the use of the benchmarks, we will organize or join challenges that aim to measure the performance of technologies for the different steps of the BLD lifecycle. In contrast to existing benchmarks, we will provide modular and easily extensible benchmarks for all industry-relevant BLD processing steps that allow to assess whole suites of software that cover more than one step.\nThe infrastructure necessary to run the evaluation campaigns will be made available. Our architecture will rely on web interfaces and cloud infrastructures to ensure scalability. The open HOBBIT platform will make human- and machine-readable, public periodic reports available. As exit strategy, the project will create an association after the second project year that will be sustained by the means of subscriptions from industry and academia and associated with existing benchmarking associations. The clear portfolio of added value for the members will be defined in the early project stages and disseminated throughout the evaluation campaigns.", 300069.0, 3223752.0, "HOBBIT", null, null, null, null, "http://www.openlinksw.com", null, "h2020_199489_998372404"], ["0XZ", "KGYAT have developed the RVCR, the world\u2019s first commercially viable Rotary Variable Compression Ratio (VCR) engine.", "Cadfem Uk Cae Ltd", "'KGYAT have developed RVCR, the world\u2019s first commercially viable Variable Compression Ratio (VCR) engine. KGYAT\u2019s proprietary RVCR (Rotary Variable Compression Ratio) engine technology combines into one high-tech powertrain the specific output and performance advantages of rotary motors and the thermal efficiency gains from VCRs. This enables the RVCR engine to eliminate the complexities in conventional crank mechanism-based VCRs and offer the advantages of; multiple fuel flexibility, efficiency gains by 30%, lower emissions, and reduced cost, making it the world\u2019s first commercially viable VCR engine. This disruptive, highly efficient fuel-flexible RVCR engine will enable a smooth transition from 20th century fossil fuel dependency to 21st century green fuels without disrupting the existing fossil-fuel based economic structures.\nRVCR has been prototyped within a laboratory environment, however KGYAT is now looking to gain funding to: demonstrate and validate the performance of a 75kW RVCR engine on a test bed for product qualification; and industrialisation of the RVCR engine with an engine manufacturer, including a design for manufacturing and assembly and the optimisation of costs, demonstrating viability of high volume production. \nDue to RVCR\u2019s robustness and efficiency it is a perfect solution to reduce emissions and fuel consumption of passenger cars.'", null, 58581.0, "RVCR", null, null, null, null, null, null, "h2020_205871_929829682"]]}}, "outwardCode": "CR0"}