{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Data for the Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Validity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukpostcodes = pd.read_csv('../postcodes/input/ukpostcodes.csv.gz')\n",
    "ukpostcodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_postcodes(df):\n",
    "    assert 'postcode' in df.columns\n",
    "    assert (~df['postcode'].isin(ukpostcodes.postcode)).sum() == 0\n",
    "    \n",
    "def validate_date_range(df):\n",
    "    assert 'start_date' in df.columns\n",
    "    assert 'end_date' in df.columns\n",
    "    assert df['start_date'].dtype == 'datetime64[ns]'\n",
    "    assert df['end_date'].dtype == 'datetime64[ns]'\n",
    "    assert (df['start_date'] > df['end_date']).sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORDIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp7_organizations = pd.read_pickle('../cordis/output/fp7_organizations.pkl.gz')\n",
    "validate_postcodes(fp7_organizations)\n",
    "fp7_organizations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp7_projects = pd.read_pickle('../cordis/output/fp7_projects.pkl.gz')\n",
    "validate_date_range(fp7_projects)\n",
    "fp7_projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp7 = pd.merge(\n",
    "    fp7_projects, fp7_organizations,\n",
    "    left_on='rcn', right_on='project_rcn', validate='1:m'\n",
    ")\n",
    "fp7['my_eu_id'] = 'fp7_' + fp7.project_rcn.astype('str') + '_' + fp7.organization_id.astype('str')\n",
    "fp7['total_cost_gbp'] = (fp7.total_cost_eur * fp7.eur_gbp).round()\n",
    "fp7['max_contribution_gbp'] = (fp7.max_contribution_eur * fp7.eur_gbp).round()\n",
    "fp7['contribution_gbp'] = (fp7.contribution_eur * fp7.eur_gbp).round()\n",
    "fp7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp7.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fp7.contribution_eur > fp7.total_cost_eur).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2020_organizations = pd.read_pickle('../cordis/output/h2020_organizations.pkl.gz')\n",
    "validate_postcodes(h2020_organizations)\n",
    "h2020_organizations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2020_projects = pd.read_pickle('../cordis/output/h2020_projects.pkl.gz')\n",
    "validate_date_range(h2020_projects)\n",
    "h2020_projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2020 = pd.merge(\n",
    "    h2020_projects, h2020_organizations,\n",
    "    left_on='rcn', right_on='project_rcn', validate='1:m'\n",
    ")\n",
    "h2020['my_eu_id'] = 'h2020_' + h2020.project_rcn.astype('str') + '_' + h2020.organization_id.astype('str')\n",
    "h2020['total_cost_gbp'] = (h2020.total_cost_eur * h2020.eur_gbp).round()\n",
    "h2020['max_contribution_gbp'] = (h2020.max_contribution_eur * h2020.eur_gbp).round()\n",
    "h2020['contribution_gbp'] = (h2020.contribution_eur * h2020.eur_gbp).round()\n",
    "\n",
    "# no briefs available for H2020\n",
    "h2020['brief_title'] = float('nan')\n",
    "h2020['teaser'] = float('nan')\n",
    "h2020['article'] = float('nan')\n",
    "h2020['image_path'] = float('nan')\n",
    "\n",
    "h2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h2020.contribution_eur > h2020.total_cost_eur).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creative Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_organisations = pd.read_pickle('../creative/output/creative_europe_organisations.pkl.gz')\n",
    "creative_organisations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_projects = pd.read_pickle('../creative/output/creative_europe_projects.pkl.gz')\n",
    "creative_projects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative = pd.merge(creative_projects, creative_organisations, on='project_number', validate='1:m')\n",
    "creative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_postcodes(creative)\n",
    "validate_date_range(creative)\n",
    "creative['max_contribution_gbp'] = (creative.max_contribution_eur * creative.eur_gbp).round()\n",
    "creative['my_eu_id'] = \\\n",
    "    'creative_' + creative.project_number + '_' + \\\n",
    "    creative.partner_number.apply('{:.0f}'.format).\\\n",
    "    str.replace('nan', 'coordinator', regex=False)\n",
    "assert creative.shape[0] == creative.my_eu_id.unique().shape[0]\n",
    "creative.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESIF (ESF/ERDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_england = pd.read_pickle('../esif/output/esif_england_2014_2020.pkl.gz')\n",
    "validate_postcodes(esif_england)\n",
    "validate_date_range(esif_england)\n",
    "esif_england.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_ni = pd.read_pickle('../esif/output/esif_ni_2014_2020.pkl.gz')\n",
    "validate_postcodes(esif_ni)\n",
    "validate_date_range(esif_ni)\n",
    "esif_ni.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_scotland = pd.read_pickle('../esif/output/esif_scotland.pkl.gz')\n",
    "validate_postcodes(esif_scotland)\n",
    "validate_date_range(esif_scotland)\n",
    "esif_scotland.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_wales = pd.read_pickle('../esif/output/esif_wales.pkl.gz')\n",
    "validate_postcodes(esif_wales)\n",
    "validate_date_range(esif_wales)\n",
    "esif_wales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_2016 = pd.read_pickle('../fts/output/fts_2016.pkl.gz')\n",
    "validate_postcodes(fts_2016)\n",
    "fts_2016['amount_gbp'] = (fts_2016.amount * fts_2016.eur_gbp).round()\n",
    "fts_2016['total_amount_gbp'] = (fts_2016.total_amount_eur * fts_2016.eur_gbp).round()\n",
    "fts_2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_2017 = pd.read_pickle('../fts/output/fts_2017.pkl.gz')\n",
    "validate_postcodes(fts_2017)\n",
    "fts_2017['amount_gbp'] = (fts_2017.amount * fts_2017.eur_gbp).round()\n",
    "fts_2017['total_amount_gbp'] = (fts_2017.total_amount_eur * fts_2017.eur_gbp).round()\n",
    "fts_2017.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erasmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erasmus_organisations = pd.read_pickle('../erasmus/output/erasmus_2017_organisations.pkl.gz')\n",
    "erasmus_organisations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erasmus_projects = pd.read_pickle('../erasmus/output/erasmus_2017_projects.pkl.gz')\n",
    "erasmus_projects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erasmus = pd.merge(erasmus_projects, erasmus_organisations, on='project_identifier', validate='1:m')\n",
    "erasmus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_postcodes(erasmus)\n",
    "\n",
    "erasmus['max_contribution_gbp'] = (erasmus.max_contribution_eur * erasmus.eur_gbp).round()\n",
    "erasmus['my_eu_id'] = \\\n",
    "    'erasmus_' + erasmus.project_identifier + '_' + \\\n",
    "    erasmus.partner_number.apply('{:.0f}'.format).\\\n",
    "    str.replace('nan', 'coordinator', regex=False)\n",
    "assert erasmus.shape[0] == erasmus.my_eu_id.unique().shape[0]\n",
    "erasmus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 1: All Points on Map, Data by District\n",
    "\n",
    "This should make the map look fairly similar to how it looks now, so it seems like a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_PLACES = [\n",
    "    (fp7, 'contribution_gbp'),\n",
    "    (h2020, 'contribution_gbp'),\n",
    "    (creative, 'max_contribution_gbp'), # TODO: split it out\n",
    "    (esif_england, 'eu_investment'),\n",
    "    (esif_ni, 'eu_investment'),\n",
    "    (esif_scotland, 'eu_investment'),\n",
    "    (esif_wales, 'eu_investment'),\n",
    "    (fts_2016.drop('amount', axis=1), 'amount_gbp'),\n",
    "    (fts_2017.drop('amount', axis=1), 'amount_gbp'),\n",
    "    (erasmus, 'max_contribution_gbp')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GeoJSON is very inefficient for representing a bunch of points, so let's use a relatively simple packed format.\n",
    "```\n",
    "min_longitude min_latitude\n",
    "outcode incode delta_longitude delta_latitude incode delta_longitude delta_latitude\n",
    "```\n",
    "We need [about 4 decimal places](https://gis.stackexchange.com/questions/8650/measuring-accuracy-of-latitude-and-longitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_outward_and_inward_codes(df):\n",
    "    df['outward_code'] = df.postcode.str.split(' ').str[0]\n",
    "    df['inward_code'] = df.postcode.str.split(' ').str[1]\n",
    "    return df\n",
    "\n",
    "def pack_geocoded_postcodes(dfs):\n",
    "    all_postcode_amounts = pd.concat([\n",
    "        df.rename(columns={amount_column: 'amount'})[['postcode', 'amount']]\n",
    "        for df, amount_column in dfs\n",
    "    ])\n",
    "    postcode_amounts = all_postcode_amounts.groupby('postcode').aggregate({'amount': sum})\n",
    "    postcode_amounts.reset_index(inplace=True)\n",
    "    postcode_amounts.amount = postcode_amounts.amount.astype('int32')\n",
    "    add_outward_and_inward_codes(postcode_amounts)\n",
    "    \n",
    "    geocoded_postcodes = pd.merge(postcode_amounts, ukpostcodes, validate='1:1')\n",
    "    \n",
    "    min_longitude = geocoded_postcodes.longitude.min()\n",
    "    min_latitude = geocoded_postcodes.latitude.min()\n",
    "    \n",
    "    geocoded_postcodes['delta_longitude'] = geocoded_postcodes.longitude - min_longitude\n",
    "    geocoded_postcodes['delta_latitude'] = geocoded_postcodes.latitude - min_latitude\n",
    "    \n",
    "    return {\n",
    "        'min_longitude': min_longitude,\n",
    "        'min_latitude': min_latitude,\n",
    "        'geocoded_postcodes': geocoded_postcodes\n",
    "    }\n",
    "\n",
    "packed_postcodes = pack_geocoded_postcodes(ALL_PLACES)\n",
    "[\n",
    "    packed_postcodes['min_longitude'],\n",
    "    packed_postcodes['min_latitude'],\n",
    "    packed_postcodes['geocoded_postcodes'].shape[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_postcodes['geocoded_postcodes'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_packed_postcode_json(packed_postcodes):\n",
    "    packed_postcodes = packed_postcodes.copy()\n",
    "   \n",
    "    grouped_postcodes = packed_postcodes['geocoded_postcodes'].\\\n",
    "        sort_values('outward_code').groupby('outward_code')\n",
    "     \n",
    "    def make_code_tuples(row):\n",
    "        coordinate = '{0:.4f}'\n",
    "        return [\n",
    "            row['inward_code'],\n",
    "            float(coordinate.format(row['delta_longitude'])),\n",
    "            float(coordinate.format(row['delta_latitude'])),\n",
    "            row['amount']\n",
    "        ]\n",
    "    \n",
    "    postcodes = {}\n",
    "    for outward_code, group in grouped_postcodes:\n",
    "        postcodes[outward_code] = [\n",
    "            x\n",
    "            for code in group.sort_values('inward_code').apply(make_code_tuples, axis=1)\n",
    "            for x in code\n",
    "        ]\n",
    "\n",
    "    min_coordinate = '{0:.6f}'\n",
    "    return {\n",
    "        'min_longitude': float(min_coordinate.format(packed_postcodes['min_longitude'])),\n",
    "        'min_latitude': float(min_coordinate.format(packed_postcodes['min_latitude'])),\n",
    "        'postcodes': postcodes\n",
    "    }\n",
    "\n",
    "with open('output/packed_postcodes.data.json', 'w') as file:\n",
    "    json.dump(make_packed_postcode_json(packed_postcodes), file, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data by District\n",
    "\n",
    "#### CORDIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump to JSON using pandas, because it puts in nulls instead of NaNs for\n",
    "# missing values. Then load the JSON into dicts for \n",
    "def make_district_data_json(df):\n",
    "    def to_json(group):\n",
    "        group.drop('outwardCode', axis=1, inplace=True)\n",
    "        return json.loads(group.to_json(orient='split', index=False))\n",
    "    return df.groupby('outwardCode').apply(to_json)\n",
    "\n",
    "def make_cordis_district_data(cordis):\n",
    "    cordis = add_outward_and_inward_codes(cordis.copy())\n",
    "\n",
    "    cordis = cordis[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'title',\n",
    "        'name', # of organization\n",
    "        'objective',\n",
    "        'contribution_gbp',\n",
    "        'total_cost_gbp',\n",
    "        'acronym',\n",
    "        'brief_title',\n",
    "        'teaser',\n",
    "        'article',\n",
    "        'project_url',\n",
    "        'organization_url',\n",
    "        'image_path',\n",
    "        'my_eu_id'\n",
    "    ]]\n",
    "    \n",
    "    cordis.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'title': 'projectTitle',\n",
    "        'name': 'organisationName',\n",
    "        'contribution_gbp': 'contribution',\n",
    "        'total_cost_gbp': 'totalCost',\n",
    "        'brief_title': 'briefTitle',\n",
    "        'project_url': 'projectUrl',\n",
    "        'organization_url': 'organizationUrl',\n",
    "        'image_path': 'imagePath',\n",
    "        'my_eu_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    return make_district_data_json(cordis)\n",
    "\n",
    "fp7_district_data = make_cordis_district_data(fp7)\n",
    "fp7_district_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2020_district_data = make_cordis_district_data(h2020)\n",
    "h2020_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creative Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_creative_district_data(creative):\n",
    "    creative = add_outward_and_inward_codes(creative.copy())\n",
    "    \n",
    "    coordinators = creative[creative.organisation_coordinator]\n",
    "    coordinators = coordinators[['project_number', 'organisation_name']]\n",
    "    creative = pd.merge(\n",
    "        creative, coordinators,\n",
    "        how='left', on='project_number', suffixes=('', '_coordinator'))\n",
    "\n",
    "    creative = creative[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'project',\n",
    "        'organisation_name',\n",
    "        'max_contribution_gbp',\n",
    "        'summary',\n",
    "        'organisation_website',\n",
    "        'organisation_name_coordinator',\n",
    "        'my_eu_id'\n",
    "    ]]\n",
    "    \n",
    "    creative.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'organisation_name': 'organisationName',\n",
    "        'max_contribution_gbp': 'maxContribution',\n",
    "        'organisation_website': 'organisationWebsite',\n",
    "        'organisation_name_coordinator': 'coordinatorName',\n",
    "        'my_eu_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    return make_district_data_json(creative)\n",
    "\n",
    "creative_district_data = make_creative_district_data(creative)\n",
    "creative_district_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_esif_district_data(esif):\n",
    "    esif = add_outward_and_inward_codes(esif.copy())\n",
    "    esif = esif[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'project',\n",
    "        'beneficiary',\n",
    "        'summary',\n",
    "        'funds',\n",
    "        'eu_investment',\n",
    "        'project_cost',\n",
    "        'my_eu_id'\n",
    "    ]]\n",
    "    \n",
    "    esif.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'project': 'projectTitle',\n",
    "        'beneficiary': 'organisationName',\n",
    "        'eu_investment': 'euInvestment',\n",
    "        'project_cost': 'projectCost',\n",
    "        'my_eu_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    return make_district_data_json(esif)\n",
    "\n",
    "esif_england_district_data = make_esif_district_data(esif_england)\n",
    "esif_england_district_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_ni_district_data = make_esif_district_data(esif_ni)\n",
    "esif_ni_district_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_scotland_district_data = make_esif_district_data(esif_scotland)\n",
    "esif_scotland_district_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_wales_district_data = make_esif_district_data(esif_wales)\n",
    "esif_wales_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_2016.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fts_district_data(fts, year):\n",
    "    fts = add_outward_and_inward_codes(fts.copy())\n",
    "    fts = fts[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'beneficiary',\n",
    "        'amount_gbp',\n",
    "        'budget_line_name_and_number',\n",
    "        'my_eu_id'\n",
    "    ]]\n",
    "    \n",
    "    fts.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'amount_gbp': 'amount',\n",
    "        'budget_line_name_and_number': 'budgetLineNameAndNumber',\n",
    "        'my_eu_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    fts['year'] = year\n",
    "    \n",
    "    return make_district_data_json(fts)\n",
    "\n",
    "fts_2016_district_data = make_fts_district_data(fts_2016, 2016)\n",
    "fts_2016_district_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_2017_district_data = make_fts_district_data(fts_2017, 2017)\n",
    "fts_2017_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erasmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_erasmus_district_data(erasmus):\n",
    "    erasmus = add_outward_and_inward_codes(erasmus.copy())\n",
    "\n",
    "    coordinators = erasmus[erasmus.organisation_coordinator]\n",
    "    coordinators = coordinators[['project_identifier', 'organisation_name']]\n",
    "    erasmus = pd.merge(\n",
    "        erasmus, coordinators,\n",
    "        how='left', on='project_identifier', suffixes=('', '_coordinator'))\n",
    "\n",
    "    erasmus = erasmus[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'project',\n",
    "        'organisation_name',\n",
    "        'max_contribution_gbp',\n",
    "        'summary',\n",
    "        'organisation_website',\n",
    "        'organisation_name_coordinator',\n",
    "        'my_eu_id'\n",
    "    ]]\n",
    "    \n",
    "    erasmus.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'organisation_name': 'organisationName',\n",
    "        'max_contribution_gbp': 'maxContribution',\n",
    "        'organisation_website': 'organisationWebsite',\n",
    "        'organisation_name_coordinator': 'coordinatorName',\n",
    "        'my_eu_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    return make_district_data_json(erasmus)\n",
    "\n",
    "erasmus_district_data = make_erasmus_district_data(erasmus)\n",
    "erasmus_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_district_data(datasets):\n",
    "    all_outward_codes = pd.concat([\n",
    "        part.reset_index().outwardCode\n",
    "        for outward_code, parts in datasets.items()\n",
    "        for part in parts\n",
    "    ]).unique()\n",
    "    \n",
    "    def merge_parts_data(parts, outward_code):\n",
    "        return {\n",
    "            'columns': parts[0].iloc[0]['columns'],\n",
    "            'data': [\n",
    "                datum\n",
    "                for part in parts\n",
    "                if (part.index == outward_code).any()\n",
    "                for datum in part[outward_code]['data']\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        outward_code: {\n",
    "            dataset: merge_parts_data(parts, outward_code)\n",
    "            for dataset, parts in datasets.items()\n",
    "            if any((part.index == outward_code).any() for part in parts)\n",
    "        }\n",
    "        for outward_code in all_outward_codes\n",
    "    }\n",
    "\n",
    "district_data = merge_district_data({\n",
    "    'cordis': [fp7_district_data, h2020_district_data],\n",
    "    'creative': [creative_district_data],\n",
    "    'esif': [\n",
    "        esif_england_district_data, esif_ni_district_data,\n",
    "        esif_scotland_district_data, esif_wales_district_data],\n",
    "    'fts': [fts_2016_district_data, fts_2017_district_data],\n",
    "    'erasmus': [erasmus_district_data]\n",
    "})\n",
    "district_data['CA4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DISTRICT_PATH = 'output/district'\n",
    "\n",
    "def list_district_data(path):\n",
    "    return glob.glob(os.path.join(path, '*.data.json'))\n",
    "\n",
    "def clear_district_data(path):\n",
    "    for f in list_district_data(path):\n",
    "        os.remove(f)\n",
    "\n",
    "def write_district_data(district_data, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    clear_district_data(path)\n",
    "    for outward_code, datasets in district_data.items():\n",
    "        output_pathname = os.path.join(path, outward_code + '.data.json')\n",
    "        with open(output_pathname, 'w') as file:\n",
    "            json.dump({\n",
    "                'outwardCode': outward_code,\n",
    "                'datasets': datasets\n",
    "            }, file, sort_keys=True)\n",
    "write_district_data(district_data, OUTPUT_DISTRICT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_district_data_stats():\n",
    "    files = list_district_data(OUTPUT_DISTRICT_PATH)\n",
    "    return pd.DataFrame({\n",
    "        'file': [file for file in files],\n",
    "        'byte_size': [os.stat(file).st_size for file in files]\n",
    "    })\n",
    "district_data_stats = find_district_data_stats()\n",
    "district_data_stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_data_stats.byte_size.sum() / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_data_stats[district_data_stats.byte_size > 1024*1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_district_data_stats().describe().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Index\n",
    "\n",
    "Generate a JS file that webpack can use to make paths for all of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_district_data_js():\n",
    "    data_files = list_district_data(OUTPUT_DISTRICT_PATH)\n",
    "    \n",
    "    def make_require(data_file):\n",
    "        basename = os.path.basename(data_file)\n",
    "        pathname = os.path.join('.', 'district', basename)\n",
    "        outward_code = basename.split('.')[0]\n",
    "        return \"  {}: require('{}')\".format(outward_code, pathname)\n",
    "\n",
    "    with open('output/district.js', 'w') as file:\n",
    "        file.write('// NB: This file is generated automatically. Do not edit.\\n')\n",
    "        file.write('export default {\\n')\n",
    "        requires = [\n",
    "            make_require(data_file)\n",
    "            for data_file in data_files\n",
    "        ]\n",
    "        file.write(',\\n'.join(requires))\n",
    "        file.write('\\n}\\n')\n",
    "write_district_data_js()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
