{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Data for the Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Validity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukpostcodes = pd.read_csv('../postcodes/input/ukpostcodes.csv')\n",
    "ukpostcodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_postcodes(df):\n",
    "    assert 'postcode' in df.columns\n",
    "    assert (~df['postcode'].isin(ukpostcodes.postcode)).sum() == 0\n",
    "    \n",
    "def validate_date_range(df):\n",
    "    assert 'start_date' in df.columns\n",
    "    assert 'end_date' in df.columns\n",
    "    assert df['start_date'].dtype == 'datetime64[ns]'\n",
    "    assert df['end_date'].dtype == 'datetime64[ns]'\n",
    "    assert (df['start_date'] > df['end_date']).sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Data\n",
    "\n",
    "### CAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_by_area = pd.read_pickle('../cap/output/cap_by_area.pkl.gz')\n",
    "cap_by_area.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORDIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp7_organizations = pd.read_pickle('../cordis/output/fp7_organizations.pkl.gz')\n",
    "fp7_organizations['my_eu_id'] = 'fp7_organization_' + fp7_organizations.organizationId.astype('str')\n",
    "validate_postcodes(fp7_organizations)\n",
    "fp7_organizations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp7_projects = pd.read_pickle('../cordis/output/fp7_projects.pkl.gz')\n",
    "fp7_projects['my_eu_id'] = 'fp7_project_' + fp7_projects.rcn.astype('str')\n",
    "fp7_projects.rename({\n",
    "    'startDate': 'start_date',\n",
    "    'endDate': 'end_date',\n",
    "}, axis=1, inplace=True)\n",
    "validate_date_range(fp7_projects)\n",
    "fp7_projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp7 = pd.merge(\n",
    "    fp7_projects, fp7_organizations,\n",
    "    left_on='rcn', right_on='projectRcn', validate='1:m'\n",
    ")\n",
    "fp7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp7.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fp7.ecContribution > fp7.totalCost).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp7.title.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESIF (ESF/ERDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_england = pd.read_pickle('../esif/output/esif_england_2014_2020.pkl.gz')\n",
    "validate_postcodes(esif_england)\n",
    "validate_date_range(esif_england)\n",
    "esif_england.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_scotland = pd.read_pickle('../esif/output/esif_scotland.pkl.gz')\n",
    "validate_postcodes(esif_scotland)\n",
    "validate_date_range(esif_scotland)\n",
    "esif_scotland.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_wales = pd.read_pickle('../esif/output/esif_wales.pkl.gz')\n",
    "validate_postcodes(esif_wales)\n",
    "validate_date_range(esif_wales)\n",
    "esif_wales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 1: All Points on Map, Data by District\n",
    "\n",
    "This should make the map look fairly similar to how it looks now, so it seems like a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_PLACES = [\n",
    "    fp7_organizations,\n",
    "    esif_england,\n",
    "    esif_scotland,\n",
    "    esif_wales\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GeoJSON is very inefficient for representing a bunch of points, so let's use a relatively simple packed format.\n",
    "```\n",
    "min_longitude min_latitude\n",
    "outcode incode delta_longitude delta_latitude incode delta_longitude delta_latitude\n",
    "```\n",
    "We need [about 4 decimal places](https://gis.stackexchange.com/questions/8650/measuring-accuracy-of-latitude-and-longitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_outward_and_inward_codes(df):\n",
    "    df['outward_code'] = df.postcode.str.split(' ').str[0]\n",
    "    df['inward_code'] = df.postcode.str.split(' ').str[1]\n",
    "    return df\n",
    "\n",
    "def pack_geocoded_postcodes(dfs):\n",
    "    postcodes = pd.DataFrame({\n",
    "        'postcode': pd.concat([df['postcode'] for df in dfs]).unique()\n",
    "    })\n",
    "    add_outward_and_inward_codes(postcodes)\n",
    "    \n",
    "    geocoded_postcodes = pd.merge(postcodes, ukpostcodes, validate='1:1')\n",
    "    \n",
    "    min_longitude = geocoded_postcodes.longitude.min()\n",
    "    min_latitude = geocoded_postcodes.latitude.min()\n",
    "    \n",
    "    geocoded_postcodes['delta_longitude'] = geocoded_postcodes.longitude - min_longitude\n",
    "    geocoded_postcodes['delta_latitude'] = geocoded_postcodes.latitude - min_latitude\n",
    "    \n",
    "    return {\n",
    "        'min_longitude': min_longitude,\n",
    "        'min_latitude': min_latitude,\n",
    "        'geocoded_postcodes': geocoded_postcodes\n",
    "    }\n",
    "\n",
    "packed_postcodes = pack_geocoded_postcodes(ALL_PLACES)\n",
    "[\n",
    "    packed_postcodes['min_longitude'],\n",
    "    packed_postcodes['min_latitude'],\n",
    "    packed_postcodes['geocoded_postcodes'].shape[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_postcodes['geocoded_postcodes'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_packed_postcodes(packed_postcodes, file):\n",
    "    datum = [packed_postcodes['min_longitude'], packed_postcodes['min_latitude']]\n",
    "    file.write(' '.join(['{0:.6f}'.format(coordinate) for coordinate in datum]))\n",
    "    file.write('\\n')\n",
    "    \n",
    "    grouped_postcodes = packed_postcodes['geocoded_postcodes'].\\\n",
    "        sort_values('outward_code').groupby('outward_code')\n",
    "\n",
    "    def write_code_tuples(row):\n",
    "        coordinate = '{0:.4f}'\n",
    "        file.write(' ')\n",
    "        file.write(' '.join([\n",
    "            row['inward_code'],\n",
    "            coordinate.format(row['delta_longitude']),\n",
    "            coordinate.format(row['delta_latitude'])\n",
    "        ]))\n",
    "\n",
    "    for outward_code, group in grouped_postcodes:\n",
    "        file.write(outward_code)\n",
    "        group.sort_values('inward_code').apply(write_code_tuples, axis=1)\n",
    "        file.write('\\n')\n",
    "\n",
    "# with open('output/packed_postcodes.txt', 'w') as file:\n",
    "#     write_packed_postcodes(packed_postcodes, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the same sort of format in JSON for comparison. It actually compresses down to about the same, and it will probably parse faster, so we might as well go with this JSON rather than the harder parsing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_packed_postcode_json(packed_postcodes):\n",
    "    packed_postcodes = packed_postcodes.copy()\n",
    "   \n",
    "    grouped_postcodes = packed_postcodes['geocoded_postcodes'].\\\n",
    "        sort_values('outward_code').groupby('outward_code')\n",
    "     \n",
    "    def make_code_tuples(row):\n",
    "        coordinate = '{0:.4f}'\n",
    "        return [\n",
    "            row['inward_code'],\n",
    "            float(coordinate.format(row['delta_longitude'])),\n",
    "            float(coordinate.format(row['delta_latitude']))\n",
    "        ]\n",
    "    \n",
    "    postcodes = {}\n",
    "    for outward_code, group in grouped_postcodes:\n",
    "        postcodes[outward_code] = [\n",
    "            x\n",
    "            for code in group.sort_values('inward_code').apply(make_code_tuples, axis=1)\n",
    "            for x in code\n",
    "        ]\n",
    "\n",
    "    min_coordinate = '{0:.6f}'\n",
    "    return {\n",
    "        'min_longitude': float(min_coordinate.format(packed_postcodes['min_longitude'])),\n",
    "        'min_latitude': float(min_coordinate.format(packed_postcodes['min_latitude'])),\n",
    "        'postcodes': postcodes\n",
    "    }\n",
    "\n",
    "with open('output/packed_postcodes.data.json', 'w') as file:\n",
    "    json.dump(make_packed_postcode_json(packed_postcodes), file, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data by District\n",
    "\n",
    "#### CORDIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump to JSON using pandas, because it puts in nulls instead of NaNs for\n",
    "# missing values. Then load the JSON into dicts for \n",
    "def make_district_data_json(df):\n",
    "    def to_json(group):\n",
    "        group.drop('outwardCode', axis=1, inplace=True)\n",
    "        return json.loads(group.to_json(orient='split', index=False))\n",
    "    return df.groupby('outwardCode').apply(to_json)\n",
    "\n",
    "def make_cordis_district_data(cordis):\n",
    "    cordis = add_outward_and_inward_codes(cordis.copy())\n",
    "\n",
    "    cordis = cordis[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'title',\n",
    "        'name', # of organization\n",
    "        'objective',\n",
    "        'ecContribution',\n",
    "        'totalCost',\n",
    "        'acronym',\n",
    "        'briefTitle',\n",
    "        'teaser',\n",
    "        'article',\n",
    "        'projectUrl',\n",
    "        'organizationUrl',\n",
    "        'imageUri'\n",
    "    ]]\n",
    "    \n",
    "    cordis.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'title': 'projectTitle',\n",
    "        'name': 'organisationName',\n",
    "        'imageUri': 'imageUrl'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    return make_district_data_json(cordis)\n",
    "\n",
    "print(fp7[fp7.postcode == 'CA4 9QY'])\n",
    "\n",
    "fp7_district_data = make_cordis_district_data(fp7)\n",
    "fp7_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_esif_district_data(esif):\n",
    "    esif = add_outward_and_inward_codes(esif.copy())\n",
    "    esif = esif[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'project',\n",
    "        'beneficiary',\n",
    "        'summary',\n",
    "        'funds',\n",
    "        'eu_investment',\n",
    "        'project_cost'\n",
    "    ]]\n",
    "    \n",
    "    esif.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'project': 'projectTitle',\n",
    "        'beneficiary': 'organisationName',\n",
    "        'eu_investment': 'euInvestment',\n",
    "        'project_cost': 'projectCost'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    return make_district_data_json(esif)\n",
    "\n",
    "esif_england_district_data = make_esif_district_data(esif_england)\n",
    "esif_england_district_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_scotland_district_data = make_esif_district_data(esif_scotland)\n",
    "esif_scotland_district_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_wales_district_data = make_esif_district_data(esif_wales)\n",
    "esif_wales_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_district_data(datasets):\n",
    "    all_outward_codes = pd.concat([\n",
    "        part.reset_index().outwardCode\n",
    "        for outward_code, parts in datasets.items()\n",
    "        for part in parts\n",
    "    ]).unique()\n",
    "    \n",
    "    return {\n",
    "        outward_code: {\n",
    "            dataset: part[part.index == outward_code][0]\n",
    "            for dataset, parts in datasets.items()\n",
    "            for part in parts\n",
    "            if (part.index == outward_code).any()\n",
    "        }\n",
    "        for outward_code in all_outward_codes\n",
    "    }\n",
    "\n",
    "district_data = merge_district_data({\n",
    "    'cordis': [fp7_district_data],\n",
    "    'esif': [esif_england_district_data, esif_scotland_district_data, esif_wales_district_data]\n",
    "})\n",
    "district_data['CA4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DISTRICT_PATH = 'output/district'\n",
    "\n",
    "def list_district_data(path):\n",
    "    return glob.glob(os.path.join(path, '*.data.json'))\n",
    "\n",
    "def clear_district_data(path):\n",
    "    for f in list_district_data(path):\n",
    "        os.remove(f)\n",
    "\n",
    "def write_district_data(district_data, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    clear_district_data(path)\n",
    "    for outward_code, datasets in district_data.items():\n",
    "        output_pathname = os.path.join(path, outward_code + '.data.json')\n",
    "        with open(output_pathname, 'w') as file:\n",
    "            json.dump({\n",
    "                'outwardCode': outward_code,\n",
    "                'datasets': datasets\n",
    "            }, file, sort_keys=True)\n",
    "write_district_data(district_data, OUTPUT_DISTRICT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_district_data_stats():\n",
    "    files = list_district_data(OUTPUT_DISTRICT_PATH)\n",
    "    return pd.DataFrame({\n",
    "        'file': [file for file in files],\n",
    "        'byte_size': [os.stat(file).st_size for file in files]\n",
    "    })\n",
    "district_data_stats = find_district_data_stats()\n",
    "district_data_stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_data_stats[district_data_stats.byte_size > 1024*1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_district_data_stats().describe().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Index\n",
    "\n",
    "Generate a JS file that webpack can use to make paths for all of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_district_data_js():\n",
    "    data_files = list_district_data(OUTPUT_DISTRICT_PATH)\n",
    "    \n",
    "    def make_require(data_file):\n",
    "        basename = os.path.basename(data_file)\n",
    "        pathname = os.path.join('.', 'district', basename)\n",
    "        outward_code = basename.split('.')[0]\n",
    "        return \"  {}: require('{}')\".format(outward_code, pathname)\n",
    "\n",
    "    with open('output/district.js', 'w') as file:\n",
    "        file.write('// NB: This file is generated automatically. Do not edit.\\n')\n",
    "        file.write('export default {\\n')\n",
    "        requires = [\n",
    "            make_require(data_file)\n",
    "            for data_file in data_files\n",
    "        ]\n",
    "        file.write(',\\n'.join(requires))\n",
    "        file.write('\\n}\\n')\n",
    "write_district_data_js()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 2: Aggregate over each Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: an area map with \n",
    "\n",
    "# is it worth splitting out the 'projects' from the 'places'?\n",
    "# we could denormalize to region level --- include all projects in an area in the json blob for that area\n",
    "# what to do with the ESIF data? It's already mostly denormalized. There is not much to deduplicate anyway.\n",
    "# Maybe just a special case for CORDIS (or other things with multiple partners per project)\n",
    "\n",
    "    \n",
    "# go through by postcode area\n",
    "# find all the things in that area\n",
    "# group them by postcode\n",
    "# for each postcode, write out a list of projects in that postcode\n",
    "# for CORDIS, maybe just have a separate cordis_projects.json file with the data?\n",
    "# I guess that might be too large... but we could split it up on rcn, for example.\n",
    "# or denormalize it and just stuff it in with the rest... maybe that's the place to start.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
